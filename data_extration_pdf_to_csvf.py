# -*- coding: utf-8 -*-
"""data_extration_pdf_to_csvf.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14-b77DMkigqlu1WDswJicxX1AUFoFJOS
"""

import os
import subprocess

!pip install pymupdf tqdm ollama

!curl -fsSL https://ollama.com/install.sh | sh

ollama_model_id = "llama3.1:8b"
os.environ["OLLAMA_HOST"] = "0.0.0.0:2000"

!nohup bash -c "OLLAMA_HOST=0.0.0.0:2000 OLLAMA_ORIGIN=* ollama serve" &
!sleep 10 && tail /content/nohup.out

!bash -c "OLLAMA_HOST=0.0.0.0:2000 OLLAMA_ORIGIN=* ollama pull {ollama_model_id}"

!nohup bash -c "OLLAMA_HOST=0.0.0.0:2000 OLLAMA_ORIGIN=* ollama run {ollama_model_id}" &
!sleep 10 && tail /content/nohup.out

import os
import fitz
import csv
import time
import logging
from tqdm import tqdm
import ollama

PDF_PATHS = [
    "/content/freezer.pdf",
    "/content/zanussi.pdf"

]

OUTPUT_CSV = "qa_dataset +.csv"
OLLAMA_MODEL = "llama3.1:8b"
logging.basicConfig(level=logging.ERROR)

def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    return "\n".join([page.get_text() for page in doc])

def split_text(text, max_chars=1500):
    paragraphs = text.split('\n\n')
    chunks, current = [], ""
    for p in paragraphs:
        if len(current) + len(p) < max_chars:
            current += p + '\n\n'
        else:
            chunks.append(current.strip())
            current = p + '\n\n'
    if current:
        chunks.append(current.strip())
    return chunks

def build_prompt(chunk):
    return f"""You are a helpful assistant for appliance users. Based on the text below, generate questions and answers as you can  usage or instruction-related question-answer pairs.

TEXT:
\"\"\"
{chunk}
\"\"\"

Format:
Q: ...
A: ...
Q: ...
A: ...
"""

def generate_with_ollama(prompt):
    try:
        response = ollama.chat(model=OLLAMA_MODEL, messages=[
            {"role": "user", "content": prompt}
        ])
        return response['message']['content']
    except Exception as e:
        logging.error(f"[Ollama Error]: {e}")
        return "Failed to generate response."

def parse_qa_response(text):
    qa_pairs = []
    question, answer = None, None
    for line in text.strip().split('\n'):
        if line.strip().startswith("Q:"):
            question = line[2:].strip()
        elif line.strip().startswith("A:"):
            answer = line[2:].strip()
            if question and answer:
                qa_pairs.append((question, answer))
                question, answer = None, None
    return qa_pairs

def filter_qa_pairs(pairs, seen_questions):
    filtered = []
    for q, a in pairs:
        q_lower = q.lower()
        if q_lower in seen_questions:
            continue
        if len(q.strip()) < 5 or len(a.strip()) < 5:
            continue
        seen_questions.add(q_lower)
        filtered.append((q.strip(), a.strip()))
    return filtered


def process_pdfs(pdf_paths, output_csv):
    seen_questions = set()

    with open(output_csv, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['question', 'answer', 'source'])

        for pdf_path in tqdm(pdf_paths, desc="Processing PDFs"):
            try:
                text = extract_text_from_pdf(pdf_path)
                print(f"\n--- Extracted Text from {pdf_path} ---\n")
                print(text[:2000])

                chunks = split_text(text)

                for chunk in chunks:
                    prompt = build_prompt(chunk)
                    response = generate_with_ollama(prompt)
                    qa_pairs = parse_qa_response(response)
                    clean_pairs = filter_qa_pairs(qa_pairs, seen_questions)

                    for q, a in clean_pairs:
                        writer.writerow([q, a, os.path.basename(pdf_path)])

            except Exception as e:
                logging.error(f" Failed to process {pdf_path}: {e}")

if __name__ == "__main__":
    process_pdfs(PDF_PATHS, OUTPUT_CSV)
    print(f"\n QA CSV saved to: {OUTPUT_CSV}")