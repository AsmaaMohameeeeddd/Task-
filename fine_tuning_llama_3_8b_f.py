# -*- coding: utf-8 -*-
"""Fine_Tuning_LLama_3_8B f.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B4-fYb7Za4pQJI5sF9p8L53U2rFOTkLd
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import torch
# major_version, minor_version = torch.cuda.get_device_capability()
# !pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
# if major_version >= 8:
#     !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes
# else:
#     !pip install --no-deps xformers trl peft accelerate bitsandbytes
# pass

from unsloth import FastLanguageModel
import torch
max_seq_length = 2048
dtype = None
load_in_4bit = True

fourbit_models = [
    "unsloth/llama-3-8b-bnb-4bit"
]

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/llama-3-8b-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)

"""

---

"""

model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0,
    bias = "none",
    use_gradient_checkpointing = "unsloth",
    random_state = 3407,
    use_rslora = False,
    loftq_config = None,
)

import pandas as pd
import re
from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer

# Load tokenizer to get EOS token
tokenizer = AutoTokenizer.from_pretrained("unsloth/llama-3-8b-bnb-4bit")
EOS_TOKEN = tokenizer.eos_token

# Alpaca-style prompt template
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

# Load and clean your CSV data
df = pd.read_csv("/content/QA_Dataset .csv")
df = df.dropna(subset=["question", "answer"]).drop_duplicates(subset=["question", "answer"])

def clean_text(text):
    text = str(text)
    text = text.encode('utf-8', 'ignore').decode()
    text = text.strip()
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^a-zA-Z0-9.,?!()\-\'\" ]+', '', text)
    return text

df["question"] = df["question"].apply(clean_text)
df["answer"] = df["answer"].apply(clean_text)
df = df[(df["question"].str.len() > 10) & (df["answer"].str.len() > 10)]
df = df[["question", "answer"]].reset_index(drop=True)

# Train-test split
train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)

# Convert to Hugging Face Dataset
train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))
test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))
dataset = DatasetDict({"train": train_dataset, "test": test_dataset})

# Format into Alpaca-style prompts
def formatting_prompts_func(examples):
    instructions = ["Answer the question accurately."] * len(examples["question"])
    inputs = examples["question"]
    outputs = examples["answer"]
    texts = []
    for instruction, input_text, output in zip(instructions, inputs, outputs):
        text = alpaca_prompt.format(instruction, input_text, output) + EOS_TOKEN
        texts.append(text)
    return {"text": texts}

# Apply formatting
dataset = dataset.map(formatting_prompts_func, batched=True)

# Save final dataset
dataset.save_to_disk("clean_QA_dataset_alpaca_format")

from trl import SFTTrainer
from transformers import TrainingArguments
import torch

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset["train"],
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    dataset_num_proc=2,
    packing=False,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=5,
        num_train_epochs=5,                # Keep this
        # max_steps=1000,                  # OR use this instead of num_train_epochs, but not both
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        output_dir="outputs",
    ),
)

trainer_stats = trainer.train()

model.save_pretrained("electric-llama3")
tokenizer.save_pretrained("electric-llama3")

from huggingface_hub import notebook_login

notebook_login()

from huggingface_hub import HfApi, HfFolder
from transformers import AutoTokenizer, AutoModelForCausalLM

from transformers import PreTrainedModel, PreTrainedTokenizerFast

# Replace with your info
repo_id = "Asmaamaghraby/llama3-electric"

from huggingface_hub import create_repo
create_repo(repo_id, private=False)

model.push_to_hub(repo_id)
tokenizer.push_to_hub(repo_id)