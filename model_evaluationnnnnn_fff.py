# -*- coding: utf-8 -*-
"""model_Evaluationnnnnn_fff.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NK9yn1ufgC0ndBUAtEjKeFaAaf41A18p
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import torch
# major_version, minor_version = torch.cuda.get_device_capability()
# !pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
# if major_version >= 8:
#     !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes
# else:
#     !pip install --no-deps xformers trl peft accelerate bitsandbytes
# pass

from huggingface_hub import notebook_login

notebook_login()

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_id = "Asmaamaghraby/llama3-electric"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype=torch.bfloat16)

question = "Can I dispose of the electronic accessories and batteries with other household waste?"
prompt = f"{question}\n"

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

outputs = model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.01)
decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(" Model Answer:\n", decoded[len(prompt):].strip())

question = "How do I set a delay time for my washing cycle?"
prompt = f"{question}\n"

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

outputs = model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.01)
decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(" Model Answer:\n", decoded[len(prompt):].strip())

question = "How do I select the Anti-allergic function?"
prompt = f"{question}\n"

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

outputs = model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.01)
decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(" Model Answer:\n", decoded[len(prompt):].strip())

question = "How do I select the Anti-allergic function?"
prompt = f"You are an expert in electric home appliances. Answer the following question clearly and helpfully.\nQuestion: {question}\n"

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

outputs = model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.01)
decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(" Model Answer:\n", decoded[len(prompt):].strip())

question = "What should I use to clean the refrigerator and get rid of bad odors?"
prompt = f"{question}\n"

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

outputs = model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.01)
decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(" Model Answer:\n", decoded[len(prompt):].strip())

question = "how to fix my washing machine"

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

outputs = model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.01)
decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(" Model Answer:\n", decoded[len(prompt):].strip())

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import pandas as pd
from tqdm import tqdm
from sklearn.model_selection import train_test_split

model.eval()

# Load full dataset
df = pd.read_csv("/content/QA_Dataset (1).csv")

# Split into train/test
train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)

# Generate answers
answers = []
for question in tqdm(test_df['question']):
    prompt = f"{question}\n"
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    input_length = inputs["input_ids"].shape[-1]

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            temperature=0.01,
            do_sample=True
        )

    generated_tokens = outputs[0][input_length:]
    answer_only = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()
    answers.append(answer_only)

# Save to CSV
test_df['generated_answer'] = answers
test_df.to_csv("test_with_generated_answers.csv", index=False)

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
import pandas as pd
import time

!pip install evaluate

!pip install rouge_score

!pip install sacrebleu

from evaluate import load
import pandas as pd
import time

# Load dataset
df = pd.read_csv("test_with_generated_answers.csv")

# Extract answers
preds = df["generated_answer"].tolist()
answer = df["answer"].tolist()

# Measure time for computing metrics (not generation, since generation already done)
start_time = time.time()

# Load metrics
rouge = load("rouge")
bleu = load("sacrebleu")

# Compute metrics
rouge_result = rouge.compute(predictions=preds, references=answer)
bleu_result = bleu.compute(predictions=preds, references=[[ref] for ref in answer])

end_time = time.time()
total_time = end_time - start_time

# Inference latency and throughput (based on number of samples)
latency = total_time / len(preds)
throughput = len(preds) / total_time

# Print
print("Fine-Tuned Model Evaluation:")
print(f"ROUGE: {rouge_result}")
print(f"BLEU: {bleu_result}")
print(f"Latency: {latency:.3f} sec/question | Throughput: {throughput:.2f} qps")

from evaluate import load
import pandas as pd
import time

# Load dataset
df = pd.read_csv("/content/baseline_results.csv")

# Extract answers
preds = df["baseline_answer"].tolist()
answer = df["answer"].tolist()

# Measure time for computing metrics (not generation, since generation already done)
start_time = time.time()

# Load metrics
rouge = load("rouge")
bleu = load("sacrebleu")

# Compute metrics
rouge_result = rouge.compute(predictions=preds, references=answer)
bleu_result = bleu.compute(predictions=preds, references=[[ref] for ref in answer])

end_time = time.time()
total_time = end_time - start_time

# Inference latency and throughput (based on number of samples)
latency = total_time / len(preds)
throughput = len(preds) / total_time

# Print
print("baseline Model Evaluation:")
print(f"ROUGE: {rouge_result}")
print(f"BLEU: {bleu_result}")
print(f"Latency: {latency:.3f} sec/question | Throughput: {throughput:.2f} qps")