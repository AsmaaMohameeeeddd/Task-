# -*- coding: utf-8 -*-
"""AppF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xX8CofHRLOTcct6W_82N5O7-KH15WT2H
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import torch
# major_version, minor_version = torch.cuda.get_device_capability()
# !pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
# if major_version >= 8:
#     !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes
# else:
#     !pip install --no-deps xformers trl peft accelerate bitsandbytes
# pass

!pip install fastapi uvicorn pyngrok transformers accelerate

!pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
# import torch
# 
# st.set_page_config(page_title="LLaMA3 Electric Generator", layout="centered")
# st.title("Electric Devives Assistant")
# 
# @st.cache_resource()
# def load_model():
#     model_id = "Asmaamaghraby/llama3-electric"
#     tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
#     model = AutoModelForCausalLM.from_pretrained(
#         model_id,
#         torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
#         device_map="auto",
#         trust_remote_code=True
#     )
#     model.eval()
#     return pipeline(
#         "text-generation",
#         model=model,
#         tokenizer=tokenizer,
#         return_full_text=False,
#     )
# 
# generator = load_model()
# 
# system_prompt = (
#     "You are an expert assistant in electric appliances. "
#     "Answer user questions clearly and helpfully about devices like fridges, TVs, washing machines, air conditioners, etc.\n\n"
# )
# 
# user_input = st.text_area("Enter your Question:", height=150)
# 
# if st.button("Generate"):
#     if user_input.strip() == "":
#         st.warning("Please enter a question.")
#     else:
#         with st.spinner("Generating..."):
#             full_prompt = system_prompt + "User: " + user_input.strip() + "\nAssistant:"
#             with torch.inference_mode():
#                 result = generator(full_prompt, max_new_tokens=150, do_sample=False, temperature=0.7)
#                 answer = result[0]["generated_text"].strip()
#                 st.text_area("Answer:", answer, height=300)
#

!pip install pyngrok
from pyngrok import ngrok

ngrok.set_auth_token("2wYvh1o9EXhsu8w2OTl6Hm6fHdd_2E5sGoDNUL9YqhjeYc4XV")  # Replace with yours
public_url = ngrok.connect(8501)
print("Streamlit app running at:", public_url)
!streamlit run app.py &>/content/log.txt &